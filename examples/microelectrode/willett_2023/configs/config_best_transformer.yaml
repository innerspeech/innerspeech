# yaml file for configuration of the neural sequence decoder  

base:
  date: ???
  exp_name: ???
  exp_dir: ???
  output_dir: './output'
  model_name: 'test'
  log_file: 'log.txt'
  note: "test"
  config_file: 'exp_config.yaml'

# Data prep configs
dataset:
  raw_path: './data/competitionData'
  preprocessed_path: './data/preprocessed_phoneme_tran.pkl'

# Model
model:
  seqLen: 150
  maxTimeSeriesLen: 1200 
  lrStart: 0.002
  lrEnd: 0.002
  nUnits: 1024
  nLayers: 10
  nClasses: 40
  nInputFeatures: 256
  dropout: 0.4
  whiteNoiseSD: 0.8
  constantOffsetSD: 0.2
  gaussianSmoothWidth: 2.0
  strideLen: 4
  kernelLen: 64
  bidirectional: true
  l2_decay: 0.00001
  tokenizer: "phoneme"
  tokenizer_path: None
  lm_version: "train"
  lm_path: "/home/ubuntu/neural_seq_decoder/speechBCI/AnalysisExamples/lm_model/data/lang_test"
  model_type: "transformer"

eval:
  # Which portion for eval step after full training run: ["test", "competition"]
  portion: "test"
  # ["train", "3-gram"]
  lm_version: "train"

# trainer
trainer:
  seed: 42
  batch_size: 16
  ml_logging: "wandb"
  project_name: 'speechbci'
  max_epochs: 201
  gpus: 1
  num_nodes: 1
  strategy: "ddp_find_unused_parameters_true"
  precision: "16-mixed"
  cuda_visible_devices: "1"
  fast_dev_run: false

# Configure environment variables for multi nodes multi GPU DDP
cluster: 
  nodes: 1
  node_rank: 0
  world_size: 1
  master_port: 1234
  master_addr: 'einstein'